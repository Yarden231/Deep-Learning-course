# Deep Learning Course Repository  
This repository contains assignments completed for the Introduction to Deep Learning course (364-2-1071) at Ben-Gurion University of the Negev, covering **MLPs, CNNs, ResNets, Self-Attention, and Optimization techniques**.

## **Table of Contents**  
- [Assignment 1: Multi-Layer Perceptrons (MLPs)](#assignment-1-multi-layer-perceptrons-mlps)  
- [Assignment 2: Advanced Neural Network Techniques](#assignment-2-advanced-neural-network-techniques)  

---

## **Assignment 1: Multi-Layer Perceptrons (MLPs)**  
**Objective:** Implement an MLP using **NumPy**, covering matrix calculus, backpropagation, and gradient descent.  

### **Topics Covered**  
- **Matrix Calculus** – Gradients of dot products, norms, and traces.  
- **Backpropagation** – Derivations for **linear layers, activation functions, softmax, and cross-entropy loss**.  
- **NumPy Implementation** – MLP with **ReLU, Softmax, and Mini-Batch SGD**.  
- **Optimization Tradeoff** – Analyzed the impact of **learning rate vs. batch size** on model convergence and performance.  

### **Files**  
- `train_mlp_numpy.py`, `modules.py`, `mlp_numpy.py` – Implementation  
- `Assignment_1_report.pdf` – Full derivations and results  

---

## **Assignment 2: Advanced Neural Network Techniques**  
**Objective:** Explore **CNNs, ResNets, Self-Attention, and Momentum Optimization**.

### **Topics Covered**  
- **CNNs** – 1D convolutions, receptive fields, weight/bias calculations.  
- **ResNets** – Basic and Bottleneck residual blocks.  
- **Self-Attention** – Query, Key, and Value parameter calculations.  
- **Momentum in SGD** – Weighted gradient accumulation for faster convergence.  

### **Files**  
- `Assignment_2.pdf` – Detailed explanation  


