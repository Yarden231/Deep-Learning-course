# Deep Learning Course Repository  

This repository contains assignments for the Deep Learning course, covering **MLPs, CNNs, ResNets, Self-Attention, and Optimization techniques**.

## **Table of Contents**  
- [Assignment 1: Multi-Layer Perceptrons (MLPs)](#assignment-1-multi-layer-perceptrons-mlps)  
- [Assignment 2: Advanced Neural Network Techniques](#assignment-2-advanced-neural-network-techniques)  

---

## **Assignment 1: Multi-Layer Perceptrons (MLPs)**  
**Objective:** Implement an MLP using **NumPy**, covering matrix calculus, backpropagation, and gradient descent.  

### **Topics Covered**  
✔ **Matrix Calculus** – Gradients of dot products, norms, and traces.  
✔ **Backpropagation** – Derivations for **linear layers, activation functions, softmax, and cross-entropy loss**.  
✔ **NumPy Implementation** – MLP with **ReLU, Softmax, and Mini-Batch SGD**.  
✔ **Learning Rate Tuning** – Train across **9 learning rates** (from \(10^{-6}\) to \(10^2\)) and analyze results.  

### **Files**  
- `train_mlp_numpy.py`, `modules.py`, `mlp_numpy.py` – Implementation  
- `Assignment_1_report.pdf` – Full derivations and results  

---

## **Assignment 2: Advanced Neural Network Techniques**  
**Objective:** Explore **CNNs, ResNets, Self-Attention, and Momentum Optimization**.

### **Topics Covered**  
✔ **CNNs** – 1D convolutions, receptive fields, weight/bias calculations.  
✔ **ResNets** – Basic and Bottleneck residual blocks.  
✔ **Self-Attention** – Query, Key, and Value parameter calculations.  
✔ **Momentum in SGD** – Weighted gradient accumulation for faster convergence.  

### **Files**  
- `Assignment_2.pdf` – Detailed explanation  


